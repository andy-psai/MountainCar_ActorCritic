{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will successfully solve Continuous Mountain Car\n",
    "if the random policy finds the top of the hill reward (+100)\n",
    "in the first several episodes. Otherwise, it will fall into \n",
    "the bad local optimum of stopping at the bottom of the hill.\n",
    "In this case, re-start until the reward is found. \n",
    "This exploration process could be automated but is not\n",
    "in this version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Number of Steps : 534, Cumulative reward: 80.36\n",
      "Episode: 1, Number of Steps : 622, Cumulative reward: 77.92\n",
      "Episode: 2, Number of Steps : 668, Cumulative reward: 75.92\n",
      "Episode: 3, Number of Steps : 812, Cumulative reward: 70.81\n",
      "Episode: 4, Number of Steps : 478, Cumulative reward: 81.86\n",
      "Episode: 5, Number of Steps : 436, Cumulative reward: 83.01\n",
      "Episode: 6, Number of Steps : 555, Cumulative reward: 80.04\n",
      "Episode: 7, Number of Steps : 597, Cumulative reward: 78.14\n",
      "Episode: 8, Number of Steps : 489, Cumulative reward: 81.30\n",
      "Episode: 9, Number of Steps : 462, Cumulative reward: 82.85\n",
      "Episode: 10, Number of Steps : 489, Cumulative reward: 80.27\n",
      "Episode: 11, Number of Steps : 405, Cumulative reward: 84.75\n",
      "Episode: 12, Number of Steps : 415, Cumulative reward: 83.63\n",
      "Episode: 13, Number of Steps : 483, Cumulative reward: 82.29\n",
      "Episode: 14, Number of Steps : 364, Cumulative reward: 85.59\n",
      "Episode: 15, Number of Steps : 446, Cumulative reward: 83.92\n",
      "Episode: 16, Number of Steps : 321, Cumulative reward: 87.31\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-988cc989ec6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[1;31m#needed to feed delta_placeholder in actor training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             td_error = target - np.squeeze(sess.run(V, feed_dict = \n\u001b[1;32m--> 121\u001b[1;33m                         {state_placeholder: scale_state(state)})) \n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[1;31m#Update actor by minimizing loss (Actor training)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\andy_\\Downloads\\WinPython-64bit-3.6.3.0Qt5\\python-3.6.3.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\andy_\\Downloads\\WinPython-64bit-3.6.3.0Qt5\\python-3.6.3.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\andy_\\Downloads\\WinPython-64bit-3.6.3.0Qt5\\python-3.6.3.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\andy_\\Downloads\\WinPython-64bit-3.6.3.0Qt5\\python-3.6.3.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\andy_\\Downloads\\WinPython-64bit-3.6.3.0Qt5\\python-3.6.3.amd64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym  #requires OpenAI gym installed\n",
    "env = gym.envs.make(\"MountainCarContinuous-v0\") \n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "input_dims = 2\n",
    "state_placeholder = tf.placeholder(tf.float32, [None, input_dims]) \n",
    "\n",
    "def value_function(state):\n",
    "    n_hidden1 = 400  \n",
    "    n_hidden2 = 400\n",
    "    n_outputs = 1\n",
    "    \n",
    "    with tf.variable_scope(\"value_network\"):\n",
    "        init_xavier = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        hidden1 = tf.layers.dense(state, n_hidden1, tf.nn.elu, \n",
    "                                  init_xavier)\n",
    "        hidden2 = tf.layers.dense(hidden1, n_hidden2, tf.nn.elu, \n",
    "                                  init_xavier) \n",
    "        V = tf.layers.dense(hidden2, n_outputs, None, init_xavier)\n",
    "    return V\n",
    "\n",
    "\n",
    "def policy_network(state):\n",
    "    n_hidden1 = 40\n",
    "    n_hidden2 = 40\n",
    "    n_outputs = 1\n",
    "    \n",
    "    with tf.variable_scope(\"policy_network\"):\n",
    "        init_xavier = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        hidden1 = tf.layers.dense(state, n_hidden1, \n",
    "                                  tf.nn.elu, init_xavier)\n",
    "        hidden2 = tf.layers.dense(hidden1, n_hidden2, \n",
    "                                  tf.nn.elu, init_xavier)\n",
    "        mu = tf.layers.dense(hidden2, n_outputs, \n",
    "                             None, init_xavier)\n",
    "        sigma = tf.layers.dense(hidden2, n_outputs, \n",
    "                                None, init_xavier)\n",
    "        sigma = tf.nn.softplus(sigma) + 1e-5\n",
    "        norm_dist = tf.contrib.distributions.Normal(mu, sigma)\n",
    "        action_tf_var = tf.squeeze(norm_dist.sample(1), axis=0)\n",
    "        action_tf_var = tf.clip_by_value(\n",
    "            action_tf_var, env.action_space.low[0], \n",
    "            env.action_space.high[0])\n",
    "    return action_tf_var, norm_dist\n",
    "\n",
    "################################################################\n",
    "#sample from state space for state normalization\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "                                    \n",
    "state_space_samples = np.array(\n",
    "    [env.observation_space.sample() for x in range(10000)])\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(state_space_samples)\n",
    "\n",
    "#function to normalize states\n",
    "def scale_state(state):                 #requires input shape=(2,)\n",
    "    scaled = scaler.transform([state])\n",
    "    return scaled                       #returns shape =(1,2)   \n",
    "###################################################################\n",
    "\n",
    "lr_actor = 0.00002  #set learning rates\n",
    "lr_critic = 0.001\n",
    "\n",
    "# define required placeholders\n",
    "action_placeholder = tf.placeholder(tf.float32)\n",
    "delta_placeholder = tf.placeholder(tf.float32)\n",
    "target_placeholder = tf.placeholder(tf.float32)\n",
    "\n",
    "action_tf_var, norm_dist = policy_network(state_placeholder)\n",
    "V = value_function(state_placeholder)\n",
    "\n",
    "# define actor (policy) loss function\n",
    "loss_actor = -tf.log(norm_dist.prob(action_placeholder) \n",
    "                     + 1e-5) * delta_placeholder\n",
    "training_op_actor = tf.train.AdamOptimizer(\n",
    "         lr_actor, name='actor_optimizer').minimize(loss_actor)\n",
    "# define critic (state-value) loss function\n",
    "loss_critic = tf.reduce_mean(tf.squared_difference(\n",
    "                             tf.squeeze(V), target_placeholder))\n",
    "training_op_critic = tf.train.AdamOptimizer(\n",
    "        lr_critic, name='critic_optimizer').minimize(loss_critic)\n",
    "################################################################\n",
    "gamma = 0.99        #discount factor\n",
    "num_episodes = 300\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    episode_history = []\n",
    "    for episode in range(num_episodes):\n",
    "        #receive initial state from E\n",
    "        state = env.reset()   # state.shape -> (2,)\n",
    "        reward_total = 0 \n",
    "        steps = 0\n",
    "        done = False\n",
    "        while (not done):\n",
    "                \n",
    "            #Sample action according to current policy\n",
    "            #action.shape = (1,1)\n",
    "            action  = sess.run(action_tf_var, feed_dict={\n",
    "                          state_placeholder: scale_state(state)})\n",
    "            #Execute action and observe reward & next state from E\n",
    "            # next_state shape=(2,)    \n",
    "            #env.step() requires input shape = (1,)\n",
    "            next_state, reward, done, _ = env.step(\n",
    "                                    np.squeeze(action, axis=0)) \n",
    "            steps +=1\n",
    "            reward_total += reward\n",
    "            #V_of_next_state.shape=(1,1)\n",
    "            V_of_next_state = sess.run(V, feed_dict = \n",
    "                    {state_placeholder: scale_state(next_state)})  \n",
    "            #Set TD Target\n",
    "            #target = r + gamma * V(next_state)     \n",
    "            target = reward + gamma * np.squeeze(V_of_next_state) \n",
    "            \n",
    "            # td_error = target - V(s)\n",
    "            #needed to feed delta_placeholder in actor training\n",
    "            td_error = target - np.squeeze(sess.run(V, feed_dict = \n",
    "                        {state_placeholder: scale_state(state)})) \n",
    "            \n",
    "            #Update actor by minimizing loss (Actor training)\n",
    "            _, loss_actor_val  = sess.run(\n",
    "                [training_op_actor, loss_actor], \n",
    "                feed_dict={action_placeholder: np.squeeze(action), \n",
    "                state_placeholder: scale_state(state), \n",
    "                delta_placeholder: td_error})\n",
    "            #Update critic by minimizinf loss  (Critic training)\n",
    "            _, loss_critic_val  = sess.run(\n",
    "                [training_op_critic, loss_critic], \n",
    "                feed_dict={state_placeholder: scale_state(state), \n",
    "                target_placeholder: target})\n",
    "            \n",
    "            state = next_state\n",
    "            #end while\n",
    "        episode_history.append(reward_total)\n",
    "        print(\"Episode: {}, Number of Steps : {}, \n",
    "              Cumulative reward: {:0.2f}\".format(episode, steps, reward_total))\n",
    "        \n",
    "        if np.mean(episode_history[-100:]) > 90 and len(episode_history) >= 101:\n",
    "            print(\"****************Solved***************\")\n",
    "            print(\"Mean cumulative reward over 100 episodes:{:0.2f}\" .format(\n",
    "                np.mean(episode_history[-100:])))\n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
